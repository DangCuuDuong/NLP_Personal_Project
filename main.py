import os
os.environ["STREAMLIT_WATCHER_TYPE"] = "none"

import streamlit as st
import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
from crawl import crawl_amazon_product_reviews  # HÃ m crawl Ä‘á»‹nh nghÄ©a riÃªng
from CleanData import Tokenization, StopWord, StemmingPorter, StemmingSnowball, Lemmatization, PosTagging, PosTaggingChart, Contraction, spellchecker, Ner, NerRender, remove_emoji
from GenData import NLPInsert, NLPSplit, NLPSubstitute, NLPSwap, NLPKeyboard, NLPBackTranslate, NLPReserved, NLPSynonym
from vectorize import OnehotEncoding, BagofWord, BagofN_Gram, TF_IDF, Word2Vec, FastText
from training import train_knn, train_decision_tree, train_svm, train_logistic_regression
from sklearn.metrics import classification_report, confusion_matrix
from training import train_knn, train_decision_tree, train_svm, train_logistic_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import openai
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler
client = openai.OpenAI(api_key)
st.set_page_config(page_title="NLP PROCESS", layout="centered")
# ========== TAB GIAO DIá»†N ==========
tab1, tab2, tab3 = st.tabs(["ğŸš€ Huáº¥n luyá»‡n & Dá»± Ä‘oÃ¡n", " ğŸ¤–chatbot"," Äá» xuáº¥t phim"])
with tab1:

    st.title("NLP PROCESS")

    # ========== 1. Nháº­p link crawl Amazon ==========
    link_craw = st.text_input("ğŸ”— Nháº­p link sáº£n pháº©m Amazon")

    if st.button("ğŸ Crawl"):
        if link_craw:
            try:
                crawl_amazon_product_reviews(link_craw)
                st.success("âœ… Crawl thÃ nh cÃ´ng!")
            except Exception as e:
                st.error(f"âŒ Lá»—i khi crawl: {e}")
        else:
            st.warning("âš ï¸ Vui lÃ²ng nháº­p link sáº£n pháº©m!")

    # ========== 2. Äá»c file crawl máº·c Ä‘á»‹nh ==========
    file_path = "amazon_reviews.csv"
    df_crawl = None

    if os.path.exists(file_path):
        try:
            df_crawl = pd.read_csv(file_path)
            st.success("ğŸ“„ ÄÃ£ Ä‘á»c file crawl amazon_reviews.csv")
            st.dataframe(df_crawl)
        except Exception as e:
            st.error(f"âŒ Lá»—i khi Ä‘á»c file crawl: {e}")

    # ========== 3. Upload file riÃªng ==========
    st.markdown("---")
    st.subheader("ğŸ“ Hoáº·c táº£i lÃªn file CSV cá»§a báº¡n")

    uploaded_file = st.file_uploader("Chá»n file CSV", type="csv")
    df_upload = None

    if uploaded_file is not None:
        try:
            df_upload = pd.read_csv(uploaded_file)
            st.success("âœ… Äá»c file ngÆ°á»i dÃ¹ng thÃ nh cÃ´ng!")
            st.dataframe(df_upload)
        except Exception as e:
            st.error(f"âŒ Lá»—i khi Ä‘á»c file táº£i lÃªn: {e}")

    # ========== 4. Chá»n cá»™t nhÃ£n vÃ  bÃ¬nh luáº­n ==========
    st.markdown("---")
    st.subheader("ğŸ§© Chá»n cá»™t Rating vÃ  Comment")

    if df_crawl is not None or df_upload is not None:
        with st.form("column_selection"):
            st.markdown("### ğŸ File crawl:")
            col_rating_crawl = st.selectbox("ğŸŸ¡ Cá»™t Rating (crawl)", df_crawl.columns if df_crawl is not None else [], key="crawl_rating")
            col_comment_crawl = st.selectbox("ğŸŸ£ Cá»™t Comment (crawl)", df_crawl.columns if df_crawl is not None else [], key="crawl_comment")

            st.markdown("### ğŸ§¾ File táº£i lÃªn:")
            col_rating_upload = st.selectbox("ğŸŸ¡ Cá»™t Rating (upload)", df_upload.columns if df_upload is not None else [], key="upload_rating")
            col_comment_upload = st.selectbox("ğŸŸ£ Cá»™t Comment (upload)", df_upload.columns if df_upload is not None else [], key="upload_comment")

            submit_cols = st.form_submit_button("âœ… Gá»™p dá»¯ liá»‡u")

        # ========== 5. Combine 2 dataframe ==========
        if submit_cols:
            try:
                df_crawl_selected = df_crawl[[col_rating_crawl, col_comment_crawl]].rename(columns={col_rating_crawl: "Rating", col_comment_crawl: "Comment"}) if df_crawl is not None else pd.DataFrame(columns=["Rating", "Comment"])
                df_upload_selected = df_upload[[col_rating_upload, col_comment_upload]].rename(columns={col_rating_upload: "Rating", col_comment_upload: "Comment"}) if df_upload is not None else pd.DataFrame(columns=["Rating", "Comment"])

                df_combined = pd.concat([df_crawl_selected, df_upload_selected], ignore_index=True)
                st.session_state["df_combined"] = df_combined  # âœ… ThÃªm dÃ²ng nÃ y

                st.success("âœ… Gá»™p dá»¯ liá»‡u thÃ nh cÃ´ng!")
                st.dataframe(df_combined)

                st.download_button(
                    label="ğŸ“¥ Táº£i file tá»•ng há»£p CSV",
                    data=df_combined.to_csv(index=False).encode('utf-8'),
                    file_name="combined_reviews.csv",
                    mime="text/csv"
                )
            except Exception as e:
                st.error(f"âŒ Lá»—i khi gá»™p dá»¯ liá»‡u: {e}")
    # ========== 6. LÃ€M Sáº CH Dá»® LIá»†U ==========
    st.markdown("---")
    st.subheader("ğŸ§¹ LÃ m sáº¡ch dá»¯ liá»‡u vÄƒn báº£n")

    # âœ… Kiá»ƒm tra vÃ  gÃ¡n láº¡i df_combined tá»« session_state
    if "df_combined" in st.session_state and not st.session_state["df_combined"].empty:
        df_combined = st.session_state["df_combined"]

        # Chá»n sá»‘ cÃ¢u Ä‘á»ƒ xá»­ lÃ½
        num_sentences = st.slider("ğŸ”¢ Sá»‘ cÃ¢u muá»‘n xá»­ lÃ½", min_value=1, max_value=len(df_combined), value=5)

        # Hiá»ƒn thá»‹ cÃ¡c cÃ¢u Ä‘áº§u tiÃªn
        st.markdown("### ğŸ“ CÃ¡c cÃ¢u sáº½ Ä‘Æ°á»£c xá»­ lÃ½")
        st.write(df_combined.head(num_sentences)[["Comment"]])

        # Chá»n cÃ¡c phÆ°Æ¡ng phÃ¡p xá»­ lÃ½
        st.markdown("### âš™ï¸ Chá»n phÆ°Æ¡ng phÃ¡p lÃ m sáº¡ch")
        clean_methods = st.multiselect(
            "Chá»n cÃ¡c bÆ°á»›c lÃ m sáº¡ch báº¡n muá»‘n Ã¡p dá»¥ng:",
            ["Contraction", "Spellcheck", "Tokenize", "Remove Stopword", 
            "Stemming (Porter)", "Stemming (Snowball)", "Lemmatization","remove_emoji", 
            "POS Tagging", "NER"],
            default=["Contraction"]
        )

        if st.button("ğŸ§¼ LÃ m sáº¡ch vÃ  thay tháº¿"):
            with st.spinner("â³ Äang xá»­ lÃ½ dá»¯ liá»‡u..."):
                cleaned_comments = []

                for i in range(num_sentences):
                    sentence = df_combined.at[i, "Comment"]

                    # BÆ°á»›c 1: Tokenize náº¿u Ä‘Æ°á»£c chá»n
                    if "Tokenize" in clean_methods:
                        sentence_list = Tokenization(sentence)
                    else:
                        sentence_list = [sentence]

                    # BÆ°á»›c 2: LÃ m sáº¡ch tá»«ng cÃ¢u
                    processed = []
                    for sent in sentence_list:
                        text = sent.text if hasattr(sent, "text") else sent

                        if "Contraction" in clean_methods:
                            text = Contraction(text)
                        if "Spellcheck" in clean_methods:
                            text = spellchecker(text)
                        if "Remove Stopword" in clean_methods:
                            text = StopWord(text)
                        if "Stemming (Porter)" in clean_methods:
                            text = StemmingPorter(text)
                        if "Stemming (Snowball)" in clean_methods:
                            text = StemmingSnowball(text)
                        if "remove_emoji" in clean_methods:
                            text = remove_emoji(text)

                        
                        processed.append(text)

                    # BÆ°á»›c 3: Káº¿t há»£p láº¡i
                    result_text = " ".join(processed)

                    # BÆ°á»›c 4: ThÃªm cÃ¡c bÆ°á»›c dáº¡ng báº£ng â†’ chuyá»ƒn thÃ nh chuá»—i náº¿u cÃ³
                    if "Lemmatization" in clean_methods:
                        df = Lemmatization(result_text)
                        result_text = " ".join(df["Lemmatization"].astype(str))

                    if "POS Tagging" in clean_methods:
                        df = PosTagging(result_text)
                        result_text = " ".join(df["TÆ°Ì€ gÃ´Ìc"].astype(str) + "/" + df["LoaÌ£i tÆ°Ì€"])

                    if "NER" in clean_methods:
                        df = Ner(result_text)
                        result_text = " ".join(df["ThÆ°Ì£c thÃªÌ‰"].astype(str) + "/" + df["GaÌn nhaÌƒn"])

                    cleaned_comments.append(result_text)

                # âœ… Cáº­p nháº­t láº¡i
                df_combined.loc[:num_sentences-1, "Comment"] = cleaned_comments
            st.success("âœ… ÄÃ£ lÃ m sáº¡ch vÃ  cáº­p nháº­t vÄƒn báº£n!")
            st.dataframe(df_combined.head(num_sentences))

    else:
        st.warning("âš ï¸ Báº¡n cáº§n gá»™p dá»¯ liá»‡u trÆ°á»›c khi lÃ m sáº¡ch vÄƒn báº£n.")

    # ========== 7. SINH Dá»® LIá»†U VÄ‚N Báº¢N ==========

    st.markdown("---")
    st.subheader("ğŸ“ˆ Sinh dá»¯ liá»‡u vÄƒn báº£n (Data Augmentation)")

    if 'df_combined' in locals() and not df_combined.empty:
        # Chá»n sá»‘ dÃ²ng Ä‘á»ƒ sinh thÃªm
        num_generate = st.slider("ğŸ”¢ Sá»‘ cÃ¢u muá»‘n sinh dá»¯ liá»‡u", min_value=1, max_value=len(df_combined), value=3)

        st.markdown("### ğŸ“ƒ CÃ¡c cÃ¢u Ä‘Æ°á»£c chá»n Ä‘á»ƒ sinh thÃªm")
        st.write(df_combined.head(num_generate)[["Comment"]])

        # Chá»n phÆ°Æ¡ng phÃ¡p sinh dá»¯ liá»‡u
        gen_methods = st.multiselect(
            "ğŸ›  Chá»n cÃ¡c phÆ°Æ¡ng phÃ¡p sinh dá»¯ liá»‡u:",
            ["Insert", "Split", "Substitute", "Swap", "Keyboard Error", "Back Translate", "Reserved Token", "Synonym Replace"],
            default=["Insert", "Substitute"]
        )

        if st.button("ğŸ“Š Sinh dá»¯ liá»‡u má»›i"):
            with st.spinner("â³ Äang sinh dá»¯ liá»‡u..."):
                new_rows = []

                for i in range(num_generate):
                    original = df_combined.at[i, "Comment"]

                    if "Insert" in gen_methods:
                        new_rows.append(NLPInsert(original))
                    if "Split" in gen_methods:
                        new_rows.append(NLPSplit(original))
                    if "Substitute" in gen_methods:
                        new_rows.append(NLPSubstitute(original))
                    if "Swap" in gen_methods:
                        new_rows.append(NLPSwap(original))
                    if "Keyboard Error" in gen_methods:
                        new_rows.append(NLPKeyboard(original))
                    if "Back Translate" in gen_methods:
                        new_rows.append(NLPBackTranslate(original))
                    if "Reserved Token" in gen_methods:
                        new_rows.append(NLPReserved(original))
                    if "Synonym Replace" in gen_methods:
                        new_rows.append(NLPSynonym(original))

                # GÃ¡n nhÃ£n cho dá»¯ liá»‡u má»›i (cÃ¹ng nhÃ£n nhÆ° báº£n gá»‘c)
                rating_col = df_combined.columns[0]  # giáº£ Ä‘á»‹nh cá»™t Ä‘áº§u lÃ  Rating
                new_data = pd.DataFrame({
                    "Rating": [df_combined.at[i % num_generate, "Rating"] for i in range(len(new_rows))],
                    "Comment": new_rows
                })

                # ThÃªm vÃ o df_combined
                df_combined = pd.concat([df_combined, new_data], ignore_index=True)
                st.success("âœ… ÄÃ£ sinh vÃ  thÃªm dá»¯ liá»‡u má»›i vÃ o!")
                st.dataframe(df_combined.tail(len(new_rows)))

                # Cáº­p nháº­t session_state náº¿u cáº§n dÃ¹ng láº¡i
                st.session_state["df_combined"] = df_combined

    # ========== 8. VECTOR HÃ“A VÄ‚N Báº¢N ==========
    st.markdown("---")
    st.subheader("ğŸ“ Vector hÃ³a vÄƒn báº£n")

    if 'df_combined' in st.session_state and not st.session_state['df_combined'].empty:
        df_combined = st.session_state['df_combined']

        num_vector = st.slider("ğŸ”¢ Sá»‘ cÃ¢u muá»‘n vector hÃ³a", min_value=1, max_value=len(df_combined), value=5, key="vec_slider")
        st.markdown("### ğŸ“ CÃ¡c cÃ¢u sáº½ Ä‘Æ°á»£c vector hÃ³a")
        st.write(df_combined.head(num_vector)[["Comment"]])

        vec_methods = st.multiselect("ğŸ§  Chá»n cÃ¡c phÆ°Æ¡ng phÃ¡p vector hÃ³a:",
            ["One-hot", "Bag of Words", "Bag of N-Gram", "TF-IDF", "Word2Vec", "FastText"], default=["TF-IDF", "Word2Vec"])

        if st.button("ğŸ“Š Vector hÃ³a vÄƒn báº£n"):
            with st.spinner("â³ Äang vector hÃ³a..."):
                result_tables = []
                vec_objects = {}
                text_list = df_combined.head(num_vector)["Comment"].tolist()

                for method in vec_methods:
                    if method == "One-hot":
                        vec, obj = OnehotEncoding(text_list)
                    elif method == "Bag of Words":
                        vec, obj = BagofWord(text_list)
                    elif method == "Bag of N-Gram":
                        vec, obj = BagofN_Gram(text_list)
                    elif method == "TF-IDF":
                        vec, obj = TF_IDF(text_list)
                    elif method == "Word2Vec":
                        vec, obj = Word2Vec(text_list)
                    elif method == "FastText":
                        vec, obj = FastText(text_list)
                    else:
                        continue
                    result_tables.append((method, vec))
                    vec_objects[method] = obj

                combined_vectors = pd.concat([v for _, v in result_tables], axis=1)
                st.dataframe(combined_vectors)

                st.session_state["vector_data"] = combined_vectors
                st.session_state["vector_rows"] = len(combined_vectors)
                st.session_state["vec_methods"] = vec_methods
                st.session_state["vec_objects"] = vec_objects

    # ========== 9. HUáº¤N LUYá»†N MÃ” HÃŒNH ==========
    st.markdown("---")
    st.subheader("ğŸ§  Huáº¥n luyá»‡n vÃ  so sÃ¡nh cÃ¡c mÃ´ hÃ¬nh phÃ¢n loáº¡i")

    if "vector_data" in st.session_state and "df_combined" in st.session_state:
        df_combined = st.session_state["df_combined"]

        label_column = st.selectbox("ğŸŸ¡ Cá»™t nhÃ£n (label)", df_combined.columns, index=0)
        test_size = st.slider("ğŸ“Š Tá»· lá»‡ test (%)", min_value=10, max_value=50, value=30) / 100

        if st.button("ğŸš€ Huáº¥n luyá»‡n táº¥t cáº£ mÃ´ hÃ¬nh"):
            with st.spinner("â³ Äang huáº¥n luyá»‡n..."):
                try:
                    X = st.session_state["vector_data"]
                    vector_rows = st.session_state["vector_rows"]
                    y = df_combined[label_column].iloc[:vector_rows]

                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

                    models = {
                        "KNN": train_knn,
                        "Decision Tree": train_decision_tree,
                        "SVM": train_svm,
                        "Logistic Regression": train_logistic_regression
                    }

                    results = []
                    trained_models = {}

                    for name, train_func in models.items():
                        model, acc, y_pred = train_func(X_train, y_train, X_test, y_test)
                        results.append((name, acc))
                        trained_models[name] = model

                        st.markdown(f"### ğŸ“Š {name}")
                        st.success(f"âœ… Accuracy: {acc:.4f}")

                        cm = confusion_matrix(y_test, y_pred)
                        fig, ax = plt.subplots()
                        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax)
                        ax.set_title(f"Ma tráº­n nháº§m láº«n - {name}")
                        ax.set_xlabel("Dá»± Ä‘oÃ¡n")
                        ax.set_ylabel("Thá»±c táº¿")
                        st.pyplot(fig)

                        st.markdown("ğŸ“‹ BÃ¡o cÃ¡o chi tiáº¿t:")
                        st.text(classification_report(y_test, y_pred))

                    # Accuracy chart
                    st.markdown("## ğŸ“ˆ So sÃ¡nh Accuracy giá»¯a cÃ¡c mÃ´ hÃ¬nh")
                    fig_acc, ax_acc = plt.subplots()
                    ax_acc.bar([n for n, _ in results], [a for _, a in results])
                    ax_acc.set_ylabel("Accuracy")
                    ax_acc.set_ylim(0, 1)
                    st.pyplot(fig_acc)

                    best_model_name, best_acc = max(results, key=lambda x: x[1])
                    best_model = trained_models[best_model_name]
                    st.session_state["best_model"] = best_model
                    st.session_state["best_model_name"] = best_model_name
                    st.success(f"ğŸ† MÃ´ hÃ¬nh tá»‘t nháº¥t: {best_model_name} (Accuracy = {best_acc:.4f})")
                except Exception as e:
                    st.error(f"âŒ Lá»—i khi huáº¥n luyá»‡n: {e}")

    # ========== 10. Dá»° ÄOÃN CÃ‚U Má»šI ==========
    st.markdown("---")
    st.subheader("ğŸ”® Dá»± Ä‘oÃ¡n tá»« mÃ´ hÃ¬nh tá»‘t nháº¥t")

    if "best_model" in st.session_state and "vec_objects" in st.session_state:
        user_input = st.text_input("ğŸ’¬ Nháº­p má»™t cÃ¢u Ä‘Ã¡nh giÃ¡ Ä‘á»ƒ dá»± Ä‘oÃ¡n:", key="predict_input")

        try:
            if user_input:
                text_list = [user_input]
                vec_parts = []
                vec_methods = st.session_state["vec_methods"]
                vec_objects = st.session_state["vec_objects"]

                for method in vec_methods:
                    vectorizer = vec_objects[method]
                    if method in ["TF-IDF", "Bag of Words", "Bag of N-Gram"]:
                        vec = vectorizer.transform(text_list)
                        vec_df = pd.DataFrame(vec.toarray(), columns=vectorizer.get_feature_names_out())
                    elif method in ["Word2Vec", "FastText"]:
                        import spacy
                        nlp = spacy.load("en_core_web_sm")
                        tokens = [token.text for token in nlp(user_input)]
                        vec_np = np.mean([vectorizer.wv[word] for word in tokens if word in vectorizer.wv] or [np.zeros(100)], axis=0)
                        vec_df = pd.DataFrame([vec_np])
                    else:
                        continue
                    vec_parts.append(vec_df)

                input_vector = pd.concat(vec_parts, axis=1)
                input_vector = input_vector.reindex(columns=st.session_state["vector_data"].columns, fill_value=0)

                predicted_label = st.session_state["best_model"].predict(input_vector)[0]
                st.success(f"ğŸ§  NhÃ£n dá»± Ä‘oÃ¡n: **{predicted_label}**")
        except Exception as e:
            st.error(f"âŒ Lá»—i khi dá»± Ä‘oÃ¡n: {e}")
    else:
        st.info("â„¹ï¸ Báº¡n cáº§n huáº¥n luyá»‡n mÃ´ hÃ¬nh trÆ°á»›c Ä‘á»ƒ sá»­ dá»¥ng chá»©c nÄƒng dá»± Ä‘oÃ¡n.")




# ğŸ‘‰ Tab giao diá»‡n chatbot
with tab2:
    st.title("DUONGPT - Trá»£ lÃ½ áº£o thÃ´ng minh")
    # 1. Khá»Ÿi táº¡o lá»‹ch sá»­ trÃ² chuyá»‡n náº¿u chÆ°a cÃ³
    user_input = st.chat_input("Nháº­p ná»™i dung trÃ² chuyá»‡n...")

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = [
            {"role": "system", "content": "ChÃ o báº¡n tÃ´i lÃ  DUONGPT - má»™t trá»£ lÃ½ áº£o thÃ´ng minh."}
        ]

    # 2. Hiá»ƒn thá»‹ tin nháº¯n trong khung cuá»™n giá»›i háº¡n chiá»u cao
    with st.container():
        chat_box = st.container()
        with chat_box:
            for msg in st.session_state.chat_history:
                with st.chat_message(msg["role"] if msg["role"] in ["user", "assistant"] else "assistant"):
                    st.markdown(msg["content"])

    # 3. Ã” nháº­p náº±m á»Ÿ dÆ°á»›i cÃ¹ng (luÃ´n xuáº¥t hiá»‡n sau pháº§n chat)
    if user_input:
        st.chat_message("user").markdown(user_input)
        st.session_state.chat_history.append({"role": "user", "content": user_input})

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=st.session_state.chat_history
        )
        reply = response.choices[0].message.content

        st.chat_message("assistant").markdown(reply)
        st.session_state.chat_history.append({"role": "assistant", "content": reply})
with tab3:
    # Load dá»¯ liá»‡u
    movies = pd.read_csv("movies.csv")
    ratings = pd.read_csv("ratings.csv")

    # Gá»™p Ä‘á»ƒ cÃ³ tÃªn phim
    df = pd.merge(ratings, movies, on="movieId")

    # Pivot: User-Movie Matrix
    user_movie_matrix = df.pivot_table(index='userId', columns='title', values='rating').fillna(0)

    # Chuáº©n hÃ³a
    scaler = StandardScaler()
    matrix_scaled = scaler.fit_transform(user_movie_matrix)

    # TÃ­nh cosine similarity giá»¯a phim
    similarity = cosine_similarity(matrix_scaled.T)  # transpose Ä‘á»ƒ so phim
    similarity_df = pd.DataFrame(similarity, index=user_movie_matrix.columns, columns=user_movie_matrix.columns)

    # Giao diá»‡n Streamlit
    st.title("ğŸ¬ Movie Recommender")

    selected_movies = st.multiselect("Chá»n phim báº¡n Ä‘Ã£ xem:", user_movie_matrix.columns)

    user_ratings = {}
    for movie in selected_movies:
        rating = st.slider(f"ÄÃ¡nh giÃ¡ cho phim '{movie}'", 1.0, 5.0, 3.0)
        user_ratings[movie] = rating

    if st.button("Äá» xuáº¥t phim"):
        scores = pd.Series(dtype="float64")
        for movie, rating in user_ratings.items():
            sim_scores = similarity_df[movie] * (rating - 2.5)  # Äiá»u chá»‰nh trung bÃ¬nh
            scores = scores.add(sim_scores, fill_value=0)

        recommended = scores.sort_values(ascending=False)
        recommended = recommended.drop(labels=selected_movies)
        st.subheader("ğŸ¯ CÃ¡c phim Ä‘á» xuáº¥t:")
        top_movies = recommended.head(10).index.tolist()
        recommendation_info = movies[movies['title'].isin(top_movies)][['title', 'genres']]

        # Gá»™p theo Ä‘Ãºng thá»© tá»± Ä‘á» xuáº¥t
        recommendation_info = recommendation_info.set_index('title').loc[top_movies].reset_index()

        for idx, row in recommendation_info.iterrows():
            st.write(f"ğŸ¬ **{row['title']}** â€” _{row['genres']}_")
